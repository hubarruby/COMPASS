{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import lxml.html\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To update data, run these next 2 cells STARTING HERE\n",
    "#then run the 4 cells on \"Plotting HASP downlinked info\"\n",
    "\n",
    "first_half_of_path = 'https://laspace.lsu.edu/hasp/groups/2021/data/'\n",
    "#a list of all the links that we need to get files/data from (we add to this)\n",
    "data_file_links = []\n",
    "#find links on this site\n",
    "connection = urllib.request.urlopen('https://laspace.lsu.edu/hasp/groups/2021/data/data.php?pname=Payload_03&py=2021')\n",
    "dom = lxml.html.fromstring(connection.read())\n",
    "\n",
    "#idk what this next line does exactly, but it gives all the links on the webpage\n",
    "for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links)\n",
    "    #find all of the links that start with \"Payload\"; these are the ones that contain files with data\n",
    "    if (link[:3] == \"Pay\") & ('old' not in link) & ('07-30' in link): #get rid of this 3rd condition for more files\n",
    "        #add the beginning of the url to the link\n",
    "        link = first_half_of_path + link\n",
    "        #add link to list of data we need to parse later on\n",
    "        data_file_links.append(link)\n",
    "# print(data_file_links)\n",
    "#need to read these files (using the links we just found) and add them together based on their timestamps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 30\n",
      "new 'file' created\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "7 30\n",
      "[1, 344, 686, 1029, 1371, 1714, 2064, 2415, 2764, 3115, 3469, 3822, 4176, 4528, 4880, 5234, 5587, 5938, 6290, 6641, 6994, 7346, 7698, 8049, 8403, 8756, 9108, 9459, 9809, 10160, 10509, 10859, 11209, 11557, 11906, 12256, 12606, 12955, 13304, 13654, 14002, 14351, 14701, 15049, 15399, 15748, 16097, 16446, 16796, 17145, 17495, 17844, 18193, 18542, 18899, 19256, 19613, 19971, 20329, 20686, 21043, 21400, 21756, 22114, 22472, 22830, 23188, 23546, 23905, 24262, 24623, 24983, 25345, 25706, 26066, 26426]\n"
     ]
    }
   ],
   "source": [
    "#combines adjacent or same days into one big file and names each big file based on the starting day\n",
    "#saves these files for later use (in a different code is probably most convenient)\n",
    "\n",
    "previous_date = datetime.datetime(2020, 12, 31) #the date each file was recorded, starting at an arbitrary value\n",
    "file_count = -1 #a count for how many big files are created\n",
    "\n",
    "#a list containing the 'title' of each file, along with the string content\n",
    "# i.e. [ [title1, content1], [title2, content2] ... ]\n",
    "all_big_files = [] \n",
    "year = 2021\n",
    "#change this depending on the size of the time gap between data files we want\n",
    "day_gap = datetime.timedelta(days = 1)\n",
    "\n",
    "for link in data_file_links:\n",
    "    # https://laspace.lsu.edu/hasp/groups/2021/data/data.php?pname=Payload_03&py=2021\n",
    "    month = int(link[61:63])\n",
    "    day = int(link[64:66])\n",
    "    date = datetime.datetime(year, month, day)\n",
    "    print(month, day)\n",
    "    \n",
    "#     #conditional for just creating the July 30th files (removable)\n",
    "#     if month == 30:\n",
    "    if date - previous_date < day_gap:\n",
    "        #write the data from this link to the current file\n",
    "        with urllib.request.urlopen(link) as f:\n",
    "            all_big_files[file_count][1] += f.read().decode('ascii', 'ignore')\n",
    "\n",
    "    else:\n",
    "        #start a new file, recording the current link date as a 'title',\n",
    "        #so it can be used to identify the file when we write them\n",
    "        with urllib.request.urlopen(link) as f:\n",
    "            new_file_start = f.read().decode('ascii', 'ignore')\n",
    "            all_big_files.append( [link[61:83], new_file_start] )\n",
    "            print(\"new 'file' created\")\n",
    "            file_count += 1 #update this counter so that we add new data to the right file later on\n",
    "\n",
    "    #change these variables for the next iteration of the loop\n",
    "    previous_date = date\n",
    "\n",
    "for i in all_big_files:\n",
    "    #setting a file name based on the date of the first entry in the file\n",
    "    new_file_name = 'HASP_downlink_' + i[0]\n",
    "    new_file_name = new_file_name.replace(\".raw\", \".csv\", 1)\n",
    "    #write everything to the .csv file\n",
    "        \n",
    "    with open(new_file_name, 'w', encoding = 'ascii') as f:\n",
    "        six_zeros_indices = [m.start() for m in re.finditer('000000', i[1])]\n",
    "        print(six_zeros_indices)\n",
    "        extra_spaces = 0\n",
    "        for j in six_zeros_indices:\n",
    "            if i[1][j - 1 + extra_spaces] != '\\n':\n",
    "                i[1] = i[1][:j + extra_spaces] + '\\n' + i[1][j + extra_spaces:]\n",
    "                extra_spaces += 1\n",
    "        i[1] = i[1].replace('\"', '%')\n",
    "        f.write(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
